{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLATIONS"
      ],
      "metadata": {
        "id": "tykzvR6EhN2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb==0.4.13 InstructorEmbedding==1.0.1 langchain==0.0.305 sentence-transformers==2.2.2 torch==2.0.1 llama-cpp-python==0.2.11 pypdf langchainhub"
      ],
      "metadata": {
        "id": "IeI4NJtlSPfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "opyfam8-jgyH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6tdXZ_-HwAh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from chromadb.config import Settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from langchain.document_loaders import DirectoryLoader, PDFMinerLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "5gSWhv1NP4pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "x1vmEVFSEiaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOOGLE DRIVE FOR SOURCE, MODEL, VECTOR DATABASE PATHS"
      ],
      "metadata": {
        "id": "xA9pxwFejnRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "parent_path = \"/content/drive/MyDrive/Langchain project\"\n",
        "source_path = os.path.join(parent_path, \"documents\")\n",
        "model_directory = os.path.join(parent_path, \"models\")\n",
        "vectordb_path = os.path.join(parent_path, \"db\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PRs8a_yLr3k",
        "outputId": "9560a200-d47a-4df3-e912-415401348794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(source_path):\n",
        "  os.makedirs(source_path)\n",
        "if not os.path.exists(model_directory):\n",
        "  os.makedirs(model_directory)\n",
        "if not os.path.exists(vectordb_path):\n",
        "  os.makedirs(vectordb_path)"
      ],
      "metadata": {
        "id": "ZMLbSmOIH3RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL = \"all-MiniLM-L12-v2\"\n",
        "MODEL_NAME = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "MODEL_FILE = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "# MODEL CONFIG\n",
        "MAX_TOKEN_LENGTH = 4094 # 8192 is the max for Mistral-7B\n",
        "N_GPU_LAYERS = 40"
      ],
      "metadata": {
        "id": "FBCEVFXbOQn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PYTORCH DEVICE COMPATIBILITY\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE_TYPE = \"cuda\"\n",
        "else:\n",
        "    DEVICE_TYPE = \"cpu\""
      ],
      "metadata": {
        "id": "RGraJe7uPxms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATABASE SETTINGS\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    is_persistent=True,\n",
        ")"
      ],
      "metadata": {
        "id": "YMN6y1bMP3I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING DOCUMENT"
      ],
      "metadata": {
        "id": "plGdvtjhkRkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_docs(directory: str = source_path):\n",
        "    \"\"\"\n",
        "    Loads documents from a specified directory.\n",
        "\n",
        "    Args:\n",
        "        directory (str): The directory path containing PDF documents.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of loaded documents.\n",
        "    \"\"\"\n",
        "    loader = DirectoryLoader(directory, glob=\"**/*.pdf\", use_multithreading=True, loader_cls= PyPDFLoader)\n",
        "    docs = loader.load()\n",
        "    logging.info(f\"Loaded {len(docs)} documents from {directory}\")\n",
        "    print(f\"Loaded {len(docs)} documents from {directory}\")\n",
        "    return docs"
      ],
      "metadata": {
        "id": "zENA02WAQl1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = load_docs(source_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaUiO0GtX6gb",
        "outputId": "8e3e1bbc-3679-4d11-c064-5076f1deb7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 718 documents from /content/drive/MyDrive/Langchain project/documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPLITTING DOCUMENT INTO CHUNKS"
      ],
      "metadata": {
        "id": "b4O2X1X-kWSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_docs(documents,chunk_size=1000,chunk_overlap=10):\n",
        "    \"\"\"\n",
        "    Splits documents into smaller chunks for processing.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents to be split.\n",
        "        chunk_size (int): The size of each chunk.\n",
        "        chunk_overlap (int): The overlap between adjacent chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: List of split documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    logging.info(f\"Split {len(documents)} documents into chunks\")\n",
        "    print(f\"Split {len(documents)} documents into {len(docs)} chunks\")\n",
        "    return docs"
      ],
      "metadata": {
        "id": "urAEAPp2RBew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = split_docs(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odu8gwx7bgRy",
        "outputId": "ef5b3a45-dadb-408f-d47f-5c2de1b849a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 718 documents into 1734 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILDING VECTOR DATABASE OF VECTOR EMBEDDINGS OF ALL CHUNKS"
      ],
      "metadata": {
        "id": "K9o2eIaMkZ-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def builder(docs):\n",
        "\n",
        "    embeddings = SentenceTransformerEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL,\n",
        "        model_kwargs={\"device\": DEVICE_TYPE},\n",
        "        cache_folder=model_directory\n",
        "    )\n",
        "    db = Chroma.from_documents(\n",
        "        docs,\n",
        "        embeddings,\n",
        "        persist_directory= vectordb_path,\n",
        "        client_settings=CHROMA_SETTINGS,\n",
        "\n",
        "    )\n",
        "    logging.info(f\"Loaded Documents to Chroma DB Successfully\")\n",
        "    print(f\"Loaded Documents to Chroma DB Successfully\")\n",
        "    return db"
      ],
      "metadata": {
        "id": "3YM9kQR2RLMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DB = builder(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7C1qRLcmYuO",
        "outputId": "44d1df86-cb1a-4a65-975f-3dca9a8eb139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Documents to Chroma DB Successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILD LOCAL LLM (LLAMA2)"
      ],
      "metadata": {
        "id": "CfNbxfgXrTfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.llms import LlamaCpp"
      ],
      "metadata": {
        "id": "A5TjiVWGYWy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
        "n_batch = 1  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/drive/MyDrive/Langchain project/models/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    n_ctx=2048,\n",
        "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDW90hPFe-oX",
        "outputId": "ea7516dd-91a4-4344-abc2-0caa23dd5fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RETRIEVAL"
      ],
      "metadata": {
        "id": "bFikCC4MvDsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=DB.as_retriever()\n",
        "retriever.get_relevant_documents(\"Pan Tompkins algorithm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-c-o20TrgMz",
        "outputId": "ac5c4a4c-258f-4faa-8b98-513a26f733e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='594 PATTERN CLASSIFICATION AND DIAGNOSTIC DECISION\\nykis calculated as\\nyk=f\\uf8eb\\n\\uf8edJ∑\\nj=1w#\\njkx#\\nj−θ#\\nk\\uf8f6\\n\\uf8f8, k= 1,2,...,K, (9.74)\\nwhere\\nx#\\nj=f(I∑\\ni=1wijxi−θj)\\n, j= 1,2,...,J, (9.75)\\nand\\nf(β) =1\\n1 + exp( −β). (9.76)\\n1\\n2\\nI1\\n2\\nJ1\\n2\\nK\\n Input layer Hidden layer Output layerw wjk ij\\ni j k##\\nx x y\\nFigure 9.4 A two\\xadlayer perceptron.\\nIn the equations given above, θjandθ#\\nkare node offsets; wijandw#\\njkare node\\nweights;xiare the elements of the pattern vectors (input parameters); and I,J, and\\nKare the numbers of nodes in the input, hidden, and output layers, respectively. The\\nweights and offsets are updated by\\nw#\\njk(n+1) =w#\\njk(n)+η[yk(1−yk)(dk−yk)]x#\\nj+α[w#\\njk(n)−w#\\njk(n−1)],(9.77)', metadata={'page': 637, 'source': '/content/drive/MyDrive/Langchain/documents/(IEEE Press series in biomedical engineering) Rangayyan, Rangaraj M - Biomedical Signal Analysis-Wiley (2015)_221104_153033 (1).pdf'}),\n",
              " Document(page_content='594 PATTERN CLASSIFICATION AND DIAGNOSTIC DECISION\\nykis calculated as\\nyk=f\\uf8eb\\n\\uf8edJ∑\\nj=1w#\\njkx#\\nj−θ#\\nk\\uf8f6\\n\\uf8f8, k= 1,2,...,K, (9.74)\\nwhere\\nx#\\nj=f(I∑\\ni=1wijxi−θj)\\n, j= 1,2,...,J, (9.75)\\nand\\nf(β) =1\\n1 + exp( −β). (9.76)\\n1\\n2\\nI1\\n2\\nJ1\\n2\\nK\\n Input layer Hidden layer Output layerw wjk ij\\ni j k##\\nx x y\\nFigure 9.4 A two\\xadlayer perceptron.\\nIn the equations given above, θjandθ#\\nkare node offsets; wijandw#\\njkare node\\nweights;xiare the elements of the pattern vectors (input parameters); and I,J, and\\nKare the numbers of nodes in the input, hidden, and output layers, respectively. The\\nweights and offsets are updated by\\nw#\\njk(n+1) =w#\\njk(n)+η[yk(1−yk)(dk−yk)]x#\\nj+α[w#\\njk(n)−w#\\njk(n−1)],(9.77)', metadata={'page': 637, 'source': '/content/drive/MyDrive/Langchain project/documents/(IEEE Press series in biomedical engineering) Rangayyan, Rangaraj M - Biomedical Signal Analysis-Wiley (2015)_221104_153033 (1).pdf'}),\n",
              " Document(page_content='˜W(k+1)=˜Wk−µk∇F(˜x), (8.132)\\nwherekis the iteration number, µis the learning rate, and ∇is a gradient measure.\\nThis procedure is a gradient descent approach for optimization (see Section 3.9.2).\\nSeveral other algorithms exist for ICA and BSS [65, 449–463]; some BSS methods\\nuse PCA as a preprocessing step.\\nIllustration of application: Zarzoso and Nandi [190] performed comparative\\nanalysis of BSS with adaptive ﬁltering (see Section 3.9.1) for the purpose of extrac-\\ntion of the fetal ECG from maternal ECG recordings. Figure 8.42 shows the input\\nECG signals used in their work, including eight channels of the maternal ECG ob-\\ntained using abdominal and thoracic surface electrodes. The result of BSS is shown\\nin Figure 8.43. Although BSS separates the mixed input signals and provides in-\\ndependent components, the method, by itself, does not identify the various sources.\\nHowever, knowing that the fetal heart rate is typically higher than the maternal heart', metadata={'page': 589, 'source': '/content/drive/MyDrive/Langchain/documents/(IEEE Press series in biomedical engineering) Rangayyan, Rangaraj M - Biomedical Signal Analysis-Wiley (2015)_221104_153033 (1).pdf'}),\n",
              " Document(page_content='˜W(k+1)=˜Wk−µk∇F(˜x), (8.132)\\nwherekis the iteration number, µis the learning rate, and ∇is a gradient measure.\\nThis procedure is a gradient descent approach for optimization (see Section 3.9.2).\\nSeveral other algorithms exist for ICA and BSS [65, 449–463]; some BSS methods\\nuse PCA as a preprocessing step.\\nIllustration of application: Zarzoso and Nandi [190] performed comparative\\nanalysis of BSS with adaptive ﬁltering (see Section 3.9.1) for the purpose of extrac-\\ntion of the fetal ECG from maternal ECG recordings. Figure 8.42 shows the input\\nECG signals used in their work, including eight channels of the maternal ECG ob-\\ntained using abdominal and thoracic surface electrodes. The result of BSS is shown\\nin Figure 8.43. Although BSS separates the mixed input signals and provides in-\\ndependent components, the method, by itself, does not identify the various sources.\\nHowever, knowing that the fetal heart rate is typically higher than the maternal heart', metadata={'page': 589, 'source': '/content/drive/MyDrive/Langchain project/documents/(IEEE Press series in biomedical engineering) Rangayyan, Rangaraj M - Biomedical Signal Analysis-Wiley (2015)_221104_153033 (1).pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RETRIEVAL QA CHAIN"
      ],
      "metadata": {
        "id": "wtHB7VM6ZiCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "Template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "Use 5 sentences maximum and keep the answer as technical as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "rag_prompt_custom = PromptTemplate.from_template(template = Template)\n",
        "chain_type_kwargs ={\"prompt\": rag_prompt_custom}"
      ],
      "metadata": {
        "id": "O8KadTnCZrZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=DB.as_retriever(),\n",
        "    chain_type=\"stuff\", chain_type_kwargs = chain_type_kwargs, return_source_documents=False, verbose = True)"
      ],
      "metadata": {
        "id": "VdnrGDW1tmF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = qa_chain.run(\"What is adaptive thresholding?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isxwTqWzcKXF",
        "outputId": "d2ae88e9-b7c0-498c-ca84-cebbbf702f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adaptive thresholding is a process that involves setting the detection threshold dynamically in response to changes in the input signal. In this case, the input signal is the segmented signal, which is generated by an AR system. The reference signal is the same as the primary input signal, but delayed by 7 samples (3.5 ms), and applied at the adaptive filter. The adaptive filter then acts as an adaptive AR model, continuously adapting its tap-weight vector to changes in the statistics of the input signal, in order to minimize the prediction error. Significant changes in the tap-weight vector or the prediction error may be used to mark points of prominent segmentation.\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ2kB_YV2Jxy",
        "outputId": "3b975e9a-0274-4a56-f36d-853317b43028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adaptive thresholding is a process that involves setting the detection threshold dynamically in response to changes in the input signal. In this case, the input signal is the segmented signal, which is generated by an AR system. The reference signal is the same as the primary input signal, but delayed by 7 samples (3.5 ms), and applied at the adaptive filter. The adaptive filter then acts as an adaptive AR model, continuously adapting its tap-weight vector to changes in the statistics of the input signal, in order to minimize the prediction error. Significant changes in the tap-weight vector or the prediction error may be used to mark points of prominent segmentation.\n"
          ]
        }
      ]
    }
  ]
}